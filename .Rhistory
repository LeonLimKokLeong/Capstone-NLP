library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
View(segmentationOriginal)
segmentationOriginal[segmentationOriginal$Case == 'Test',]
testing = segmentationOriginal[segmentationOriginal$Case == 'Test',]
training = segmentationOriginal[segmentationOriginal$Case == 'Train',]
set.seed(125)
View(training)
fit = train(Class ~ ., data = training, method = "rpart")
predict(fit, testing)
test.result = predict(fit, testing)
confusionMatrix(test.result, testing$Class)
View(testing)
questions = training
length(questions)
questions = questions[-(2:119),]
questiosn
questions
questions = questions[-c(2:119),]
questions
questions = questions[-c(2:773),]
questions
questions[1,"TotalIntench2"]
questions[1,questions$TotalIntench2]
questions[,questions$TotalIntench2]
questions[,"TotalIntench2"]
questions[,TotalIntench2]
questions[,questions$TotalIntench2]
names(questions)
questions[,103]
questions[,questions$TotalIntenCh2]
questions[,questions$TotalIntenCh2]
questions[,TotalIntenCh2]
questions[,"Class"]
questions[,"TotalIntenCh2"]
questions[2,"TotalIntenCh2"] = 23,000
questions[2,"TotalIntenCh2"] = 23000
questions[3,"TotalIntenCh2"] = 50000
questions[4,"TotalIntenCh2"] = 57000
questions[2,"FiberWidthCh1"] = 10
questions[3,"FiberWidthCh1"] = 10
questions[4,"FiberWidthCh1"] = 8
questions[2,"PerimStatusCh1"] = 2
questions[3,"VarIntenCh4"] = 100
questions[4,"VarIntenCh4"] = 100
questions[5,"FiberWidthCh1"] = 8
questions[5,"VarIntenCh4"] = 100
questions[5,"PerimStatusCh1"] = 2
questions[-c(1),]
questions = questions[c(2:5),]
questions
View(questions)
predict(fit,questions)
View(testing)
View(questions)
library(rpart)
library(ggplot2)
library(rattle)
questions[,] =""
View(questions)
View(segmentationOriginal)
rm(*)
View(training)
View(testing)
View(questions)
rm(questions)
View(training)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Class == 'Train',]
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Class == 'Train',]
testing = segmentationOriginal[segmentationOriginal$Class == 'Test',]
modFit = train( Class~. , data=training , method="rpart")
training = segmentationOriginal[segmentationOriginal$Class == "Train",]
testing = segmentationOriginal[segmentationOriginal$Class == "Test",]
View(segmentationOriginal)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
modFit = train( Class~. , data=training , method="rpart")
View(testing)
install.packages("rattle")
fancyRpartPlot(modFit$finalModel)
#########################
## Parallel Processing ##
#########################
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1) # minimum 1 core in case it can't detect the number
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
```
```{r}
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
```{r}
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
modFit = train( Class~. , data=training , method="rpart")
modPredict = predict(modFit, testing)
modMatrix = confusionMatrix(modPredict, testing$Class)
modFit$finalModel
plot(modFit$finalModel, uniform=T, main="Tree")
text(modFit$finalModel, use.n = T, all = T, cex = .8)
```
```{r}
library(rattle)
library(rpart.plot)
library(rpart)
library("rpart", lib.loc="C:/R/library")
fancyRpartPlot(modFit$finalModel)
detach("package:rpart", unload=TRUE)
install.packages("shiny")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
```
This function plots distance versus speed, each de-meaned and an associated line of slope s. Which of the following code will make a manipulate plot that creates a slider for the slope?
```{r}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
install.packages("rcharts")
install.packages("rcharts")
install.packages("rchart")
install.packages("devtools")
require(devtools)
install_github('rCharts', 'ramnathv')
library(rtools)
library(devtools)
install.packages("Rtools")
library(devtools)
find_rtools()
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
shiny::runApp('C:/Users/S7324302E/Desktop/IT Sling/2015 - Exponential Scoring/ShinyApp')
install.packages("tm")
size = 300000
inspect(dtm[1:10, 1:10])
?findFreqTerms
?tm_map
library(tm)
?tm_map
stopwords("english")
lobrary(tm)
library(tm)
stopwords("english")
removePunctuation
?removePunctuation
?replace
?removePunctuation
?removeSparseTerms
dtm = removeSparseTerms(dtm, 0.02)
inspect(dtm[1:10, 1])
findFreqTerms(dtm, lowfreq=1500) # find terms with a bounding low frequency at 1500
size = 10000
data = NULL
set.seed(1)
twitter = readLines("./data/en_US/en_US.twitter.8.txt")
data = c(data, sample(twitter, size))
rm(twitter) #clean up environment
set.seed(1)
news = readLines("./data/en_US/en_US.news.8.txt")
data = c(data, sample(news, size))
rm(news) #clean up environment
set.seed(1)
blogs = readLines("./data/en_US/en_US.blogs.8.txt")
data = c(data, sample(blogs, size))
rm(blogs) #clean up environment
setwd("C:/Users/S7324302E/Desktop/Coursera/Modules/M10 - Capstone Project")
setwd("C:/Users/S7324302E/Desktop/Coursera/Modules/M10 - Capstone Project/Milestone Project")
size = 10000
data = NULL
set.seed(1)
twitter = readLines("./data/en_US/en_US.twitter.8.txt")
data = c(data, sample(twitter, size))
rm(twitter) #clean up environment
set.seed(1)
news = readLines("./data/en_US/en_US.news.8.txt")
data = c(data, sample(news, size))
rm(news) #clean up environment
set.seed(1)
blogs = readLines("./data/en_US/en_US.blogs.8.txt")
data = c(data, sample(blogs, size))
rm(blogs) #clean up environment
profanity = readLines("bannedwordlist.txt")
data = replace(data, list = profanity, values = "")
data = replace(data, list = profanity, values = "")
data = Corpus(VectorSource(data))
# tm corpus
library(tm)
#ngrams
library(RWeka)
#wodcloud
library(wordcloud)
# plot
library(ggplot2)
data = Corpus(VectorSource(data))
dtm = TermDocumentMatrix(data)
Sdtm = removeSparseTerms(dtm, 0.998)
inspect(dtm)
inspect(Sdtm)
findFreqTerms(dtm, lowfreq=1) # find terms with a bounding low frequency at 1500
findFreqTerms(Sdtm, lowfreq=1) # find terms with a bounding low frequency at 1500
?remove
